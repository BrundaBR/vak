{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as cpio\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Define the folder that contains all data files\n",
    "# Each data file contains the variables:\n",
    "#    s: The spectrogram [size = 513 x time_steps]\n",
    "#    f: Frequencies [size = 513]\n",
    "#    t: Time steps\n",
    "#    labels: The tagging data [size = time_steps]\n",
    "data_directory = '/Users/yardenc/Documents/Experiments/Imaging/Data/CanaryData/lrb853_15/movs/wav/mat'\n",
    "# This folder must also contain a matlab file 'file_list.mat' with cell array 'keys' that holds the data file names\n",
    "data_list = cpio.loadmat(data_directory + '/file_list.mat')\n",
    "number_of_files = len(data_list['keys'][0])\n",
    "# The folder for saving training checkpoints\n",
    "training_records_dir = '/Users/yardenc/Documents/Experiments/Imaging/Data/CanaryData/lrb853_15/training_records'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "input_vec_size = lstm_size = 513\n",
    "batch_size = 2\n",
    "n_lstm_layers = 2\n",
    "n_syllables = 20 #including zero\n",
    "learning_rate = 0.001\n",
    "n_max_iter = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The inference graph\n",
    "def label_inference_graph(spectrogram, num_hidden, num_layers, seq_length):\n",
    "    # First the dynamic bi-directional, multi-layered LSTM\n",
    "    with tf.name_scope('biRNN'):\n",
    "        lstm_f = tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "        lstm_b = tf.contrib.rnn.BasicLSTMCell(num_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "        cells_f = tf.contrib.rnn.MultiRNNCell([lstm_f]*num_layers, state_is_tuple=True)\n",
    "        cells_b = tf.contrib.rnn.MultiRNNCell([lstm_b]*num_layers, state_is_tuple=True)\n",
    "        outputs, _states = tf.nn.bidirectional_dynamic_rnn(cells_f,cells_b, spectrogram, time_major=False, dtype=tf.float32,sequence_length=seq_length)\n",
    "    # Second, projection on the number of syllables creates logits tf.stack([spectrogram],axis=0)\n",
    "    with tf.name_scope('Projection'):\n",
    "        W_f = tf.Variable(tf.random_normal([num_hidden, n_syllables]))\n",
    "        W_b = tf.Variable(tf.random_normal([num_hidden, n_syllables]))\n",
    "        bias = tf.Variable(tf.random_normal([n_syllables]))\n",
    "    logits = [tf.matmul(outputs[0][:][-1],W_f) + bias + tf.matmul(outputs[1][:][-1],W_b)]# for a,b in zip(range(batch_size),range(batch_size))]\n",
    "    return logits,outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The training graph. Calculate cross entropy and loss function\n",
    "def training_graph(logits, labels, rate):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits,labels = labels, name='xentropy')\n",
    "    cost = tf.reduce_sum(xentropy, name='cost')\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = optimizer.minimize(cost, global_step=global_step)\n",
    "    return train_op, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct the full graph and add saver\n",
    "full_graph = tf.Graph()\n",
    "with full_graph.as_default():\n",
    "    # Generate placeholders for the spectrograms and labels.\n",
    "    X = tf.placeholder(\"float\", [None,None,input_vec_size], name = \"Xdata\") # holds spectrograms\n",
    "    Y = tf.placeholder(\"int32\",[None,None],name = \"Ylabels\") # holds labels\n",
    "    lng = tf.placeholder(\"int32\",name = \"nSteps\") # holds the sequence length\n",
    "                                    \n",
    "    tf.add_to_collection(\"specs\", X)  # Remember this Op.\n",
    "    tf.add_to_collection(\"labels\", Y)  # Remember this Op.\n",
    "    tf.add_to_collection(\"lng\", lng)  # Remember this Op.\n",
    "\n",
    "    # Build a Graph that computes predictions from the inference model.\n",
    "    logits,outputs = label_inference_graph(X, lstm_size, n_lstm_layers, lng)\n",
    "    tf.add_to_collection(\"logits\", logits)  # Remember this Op.\n",
    "\n",
    "    # Add to the Graph the Ops that calculate and apply gradients.\n",
    "    train_op, cost = training_graph(logits, Y, learning_rate) \n",
    "    \n",
    "    # Add the variable initializer Op.\n",
    "    init = tf.global_variables_initializer() #initialize_all_variables()\n",
    "\n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver(max_to_keep = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and save checkpoint at the end of each file.\n",
    "with tf.Session(graph=full_graph) as sess:\n",
    "    # Run the Op to initialize the variables.\n",
    "    sess.run(init)\n",
    "    # Start the training loop.\n",
    "    costs = []\n",
    "    step = 1\n",
    "    # Go over all training files\n",
    "    file_num = 0\n",
    "    fname = data_list['keys'][0][file_num][0][0:-3]+'mat'\n",
    "    data = cpio.loadmat(data_directory + '/' + fname)\n",
    "    data1 = np.transpose(data['s'])\n",
    "    intY = data['labels'][0]\n",
    "    for file_num in range(number_of_files-1):\n",
    "        # load current training file\n",
    "        fname = data_list['keys'][0][file_num+1][0][0:-3]+'mat'\n",
    "        bdata = cpio.loadmat(data_directory + '/' + fname)\n",
    "        bdata1 = np.transpose(bdata['s'])\n",
    "        bintY = bdata['labels'][0]\n",
    "        data1 = np.concatenate((data1,bdata1),axis = 0)\n",
    "        intY = np.concatenate((intY,bintY),axis = 0)\n",
    "    temp_n = len(intY)/batch_size\n",
    "    data1 = data1[0:temp_n*batch_size].reshape((batch_size,temp_n,-1))\n",
    "    intY = intY[0:temp_n*batch_size].reshape((batch_size,-1))\n",
    "    iter_order = np.random.permutation(data1.shape[1]-370)\n",
    "    if (len(iter_order) > n_max_iter):\n",
    "        iter_order = iter_order[0:n_max_iter]\n",
    "    for iternum in iter_order:\n",
    "        d = {X: data1[:,iternum:iternum+30,:] ,Y: intY[:,iternum:iternum+30] ,lng:[30]}\n",
    "        _cost,_ = sess.run((cost,train_op),feed_dict = d)\n",
    "        costs.append(_cost)\n",
    "        print([file_num, step, _cost])\n",
    "        step = step + 1\n",
    "            \n",
    "        if (step % 1000 == 0):    \n",
    "            checkpoint_file = os.path.join(training_records_dir, 'checkpoint')\n",
    "            saver.save(sess, checkpoint_file, global_step=step)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3, 513)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[:,1:4,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate training set from a saved checkpoint\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    saver = tf.train.import_meta_graph(\n",
    "        os.path.join(training_records_dir, \"checkpoint-5001.meta\"))\n",
    "    saver.restore(\n",
    "        sess, os.path.join(training_records_dir, \"checkpoint-5001\"))\n",
    "\n",
    "    # Retrieve the Ops we 'remembered'.\n",
    "    logits = tf.get_collection(\"logits\")[0]\n",
    "    X = tf.get_collection(\"specs\")[0]\n",
    "    Y = tf.get_collection(\"labels\")[0]\n",
    "    lng = tf.get_collection(\"lng\")[0]\n",
    "    \n",
    "    # Add an Op that chooses the top k predictions.\n",
    "    eval_op = tf.nn.top_k(logits)\n",
    "    \n",
    "    # Run evaluation.\n",
    "    errors = []\n",
    "    for file_num in range(number_of_files):\n",
    "        # load current training file\n",
    "        fname = data_list['keys'][0][file_num][0][0:-3]+'mat'\n",
    "        data = cpio.loadmat(data_directory + '/' + fname)\n",
    "        data1 = np.transpose(data['s'])\n",
    "        intY = data['labels'][0]\n",
    "        d = {X: data1 ,Y: intY ,lng:[len(intY)]}\n",
    "        pred = sess.run(eval_op,feed_dict = d)\n",
    "        errors.append(np.abs(np.squeeze(pred[1])-intY) != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.34048412813451223,\n",
       " 0.35495445763179684,\n",
       " 0.093001841620626149,\n",
       " 0.47276603993021904,\n",
       " 0.032034853921066121,\n",
       " 0.46023308116949502,\n",
       " 0.4215967834577829,\n",
       " 0.28068693693693691]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Results on training set\n",
    "[np.mean(err) for err in errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
